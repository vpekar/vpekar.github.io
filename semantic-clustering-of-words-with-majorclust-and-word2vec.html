<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="http://vpekar.github.io/theme/css/style.less">
  <script src="//cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="icon" type="image/vnd.microsoft.icon" href="http://vpekar.github.io/images/gs.png">
  <link rel="stylesheet" type="text/css" href="http://vpekar.github.io/theme/css/normalize.css">
  <link rel="stylesheet" type="text/css" href="http://vpekar.github.io/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="http://vpekar.github.io/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto+Mono">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">
  <link rel="stylesheet" href="http://vpekar.github.io/extras/academicons-1.8.6/css/academicons.css"/>
  <link rel="stylesheet" type="text/css" href="http://vpekar.github.io/theme/css/hatena-bookmark-icon.css">
  <link rel="stylesheet" type="text/css" href="/static/custom.css">


  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Viktor Pekar">
  <meta name="description" content="Posts and writings by Viktor Pekar">

  <link href="http://vpekar.github.io/feed/all.rss.xml" type="application/rss+xml" rel="alternate" title="Viktor Pekar RSS" />

<meta name="keywords" content="Clustering, MajorClust, word2vec, Python">

  <title>
    Viktor Pekar
&ndash; Semantic clustering of words with MajorClust and word2vec  </title>

</head>

<body>
  <main>
    <header>
      <div class="site-name">
        <a href="http://vpekar.github.io">Viktor Pekar</a>
      </div>
      <p>

      <a href="http://vpekar.github.io/pages/about.html">About</a> |
      <a href="http://vpekar.github.io/pages/research.html">Research</a> |
      <a href="http://vpekar.github.io/pages/teaching.html">Teaching</a> |
      <!--<a href="http://vpekar.github.io/pages/consultancy.html">Consulting</a> | -->
      <a href="http://vpekar.github.io/index.html">Blog</a> |
      <a href="http://vpekar.github.io/tags.html">Tags</a> |
      <a href="http://vpekar.github.io/pages/contact.html">Contact</a>

      <!--
        <a href="http://vpekar.github.io/archives.html">Archive</a>
    -->

      </p>
    </header>

<article>
  <div class="article__title">
    <h1>Semantic clustering of words with MajorClust and word2vec</h1>
  </div>
  <div class="article__meta">
    <p class="article__meta__post-date">Posted on: Sun 10 February 2019</p>
 Tags:
      <a href="http://vpekar.github.io/tag/clustering.html">#Clustering</a>,      <a href="http://vpekar.github.io/tag/majorclust.html">#MajorClust</a>,      <a href="http://vpekar.github.io/tag/word2vec.html">#word2vec</a>,      <a href="http://vpekar.github.io/tag/python.html">#Python</a>    </p>
  </div>
  <div class="article__text">
    <p>In a recent project, I needed to arrange around a thousand words into semantic clusters. For this problem, the <a href="https://en.wikipedia.org/wiki/Vector_space_model">Vector Space Model</a> seems well-suited. Previous research has released word vectors already trained on large text collections using methods like <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> or <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>. These vectors can be used to produce a similarity matrix, which can then be used for word clustering.</p>
<p>The first step is to obtain the word vectors, e.g. using the gensim library's downloader tool:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;glove-wiki-gigaword-100&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Assuming the words one wants to cluster are in <em>words</em>, keep only relevant word vectors:</p>
<div class="highlight"><pre><span></span><code><span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">vectors</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
</code></pre></div>

<p>The next step is the choice of a clustering algorithm. While there are a few popular ones like K-means and DBSCAN, <a href="https://www.semanticscholar.org/paper/Document-Categorization-with-MAJORCLUST-Stein-Eissen/2380d838f03d0564631475904dc61e4c077a2997">Majorclust</a> is somewhat less known, but has an important advantage: it does not require stopping criteria like the number of clusters, the number of elements in a cluster, or the maximum distance between elements within a cluster, to be pre-set in advance: in many NLP tasks such as word clustering these are very hard to guess, even through lots of experimentation.</p>
<p>The algorithm is described in <a href="https://www.semanticscholar.org/paper/Document-Categorization-with-MAJORCLUST-Stein-Eissen/2380d838f03d0564631475904dc61e4c077a2997">(Stein and Meyer zu Essen, 2002)</a> in application to document clustering (see p.4 for pseudocode and explanation). Below is its Python implementation taken from <a href="https://gist.github.com/baali/7983261">here</a> (<em>cosine_matrix</em> is a pre-computed matrix of document similarities):</p>
<div class="highlight"><pre><span></span><code><span class="c1"># first assign each document to its own cluster</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_of_samples</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">t</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># iterate over the documents</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_of_samples</span><span class="p">):</span>
        <span class="c1"># find the cluster with the biggest similarity to the document</span>
        <span class="n">new_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">cosine_matrix</span><span class="p">[</span><span class="n">index</span><span class="p">]))</span>
        <span class="c1"># if the cluster of the document has changed, iterate over the documents again</span>
        <span class="k">if</span> <span class="n">indices</span><span class="p">[</span><span class="n">new_index</span><span class="p">]</span> <span class="o">!=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
            <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">new_index</span><span class="p">]</span>
            <span class="n">t</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>

<p>In brief, the algorithm begins by assigning every document to its own cluster. At each iteration, each document gets assigned to the cluster, to which it has the biggest similarity. A document's similarity to a cluster is calculated as the sum of the document's cosine similarities to each of the cluster's members. Thus, a document can be re-clustered to other clusters at later iterations. Clustering stops when no document changes its cluster.</p>
<p>However I found that two modifications to this algorithm are necessary in my case. First, if many elements in the matrix have non-zero similarities to many other elements (e.g., word vectors have lots of non-zero values in the same columns, as is the case with word2vec vectors), then MajorClust may end up putting all elements into one single cluster. Therefore, it was necessary to introduce a threshold below which similarities get cancelled out, i.e. set to 0. After some experimentation I found that good results can be achieved by removing up to 99% of the smallest similarity pairs.</p>
<p>Second, after cancelling out low similarities, there will be many elements with no similarity to any other element. I.e., each of them should either constitute a cluster of its own or be treated as noise. MajorClust will merge two elements together even if the similarity between them is 0, thus producing one large catch-all cluster with lots of unrelated words. So the other modification is to prevent assigning an element to a cluster if its similarity equals zero.</p>
<p>The implementation below requires that all similarity values are equal or greater than 0, so first the features in the vectors need to be scaled to a range, e.g. (0, 1):</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">minmax_scale</span>

<span class="n">vectors</span> <span class="o">=</span> <span class="n">minmax_scale</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<p>A similarity matrix can now be produced:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="k">def</span> <span class="nf">get_sim_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Output pairwise cosine similarities in X. Cancel out the bottom</span>
<span class="sd">    `threshold` percent of the pairs sorted by similarity.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="n">sorted_sims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">sim_matrix</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="n">thr_val</span> <span class="o">=</span> <span class="n">sorted_sims</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">sorted_sims</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">threshold</span><span class="p">)]</span>
    <span class="n">sim_matrix</span><span class="p">[</span><span class="n">sim_matrix</span> <span class="o">&lt;</span> <span class="n">thr_val</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">return</span> <span class="n">sim_matrix</span>

<span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">get_sim_matrix</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</code></pre></div>

<p>The <em>threshold</em> parameter in the function represents the bottom percent of pairs to be cancelled out, after sorting the pairs by the cosine measure.</p>
<p>Now run Majorclust on the similarity matrix:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">majorclust</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Actual MajorClust algorithm</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">t</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sim_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">t</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sim_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="c1"># check if all the sims of the word are not zeros</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">sim_matrix</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="c1"># aggregating edge weights</span>
            <span class="n">new_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">indices</span><span class="p">[</span><span class="n">new_index</span><span class="p">]</span> <span class="o">!=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]:</span>
                <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">new_index</span><span class="p">]</span>
                <span class="n">t</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">indices</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">majorclust</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">)</span>
<span class="n">word2clusters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
</code></pre></div>

<p>The clustering result is <em>word2clusters</em>, a dictionary with words and their cluster ids.</p>
<p>With the modifications above, MajorClust came up with quite reasonable clusters like:</p>
<ul>
<li>
<p><em>car, vehicle, bike, truck, drive, driver, bus, helmet, cycle, motorcycle, pickup</em></p>
</li>
<li>
<p><em>game, xbox, player, warfare, cod, dlc, battlefield, playstation, wii, nintendo, fifa, controller, sims, console, play, titanfall, ball, skyrim, gta, halo, gaming, ops, code, madden</em></p>
</li>
<li>
<p><em>ford, toyota, bmw, audi, honda, tesla, nissan, jeep, kia, suv, ferrari, range, mustang, chevy, subaru, skoda, lexus, porsche, lamborghini, hyundai, moto, mercedes, gmc, benz, rover, chevrolet, mazda, mower, golf, merc, jaguar, fords, turbo, tractor, duster, diesel, camry, gtr, volvo, auto, corvette, renault, dodge, datsun, volkswagen</em></p>
</li>
<li>
<p><em>carrier, boat, flight, baggage, plane, drone, air, jet, back, trip, train, team, aircraft, balloon, yacht, space, cabin, fleet</em></p>
</li>
<li>
<p><em>gun, datum, weapon, van, mag, ben, rifle, firearm, ammo, shotgun</em></p>
</li>
<li>
<p><em>influence, supply, gas, power, petrol, fuel, electricity, boost, focus, booster, energy</em></p>
</li>
</ul>
<p>Examples of words that ended up in singleton clusters: <em>thing, brand, note, coin, item, box, case, pack, set, time, switch, machine</em>. As one can see, these are words with many different meanings, so that it is hard to assign them to any particular semantic cluster.</p>
<p>Below is a t-SNE visualization of the clustered words (click the image for a zoomable graph).</p>
<p><a href="./pages/majorclustviz.html"><img alt="alt text" src="./images/majorclust1.png" title="Click for an interactive graph"></a></p>
<p>The implementation of MajorClust used to derive these clusters, along with the scikit-learn API, can be found <a href="https://github.com/vpekar/sklearn-majorclust">here</a>.</p>
  </div>
<section class="social-tools">

    <div class="social-tools__title">
      <p>Share this post</p>
    </div>

    <div class="social-tools__buttons">
      <a class="tweet-blogpost-button"
         href="https://twitter.com/share?text=Viktor Pekar - Semantic clustering of words with MajorClust and word2vec"
         onClick="window.open(encodeURI(decodeURI(this.href)), 'tweetwindow', 'width=650, height=470'); return false;" >
        <i class="fab fa-twitter"></i>
      </a>

      <a class="facebook-share-button"
         href="https://www.facebook.com/sharer/sharer.php?u=http://vpekar.github.io/semantic-clustering-of-words-with-majorclust-and-word2vec.html"
         onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" >
        <i class="fab fa-facebook"></i>
      </a>

      <a class="pocket-add-button"
         href="http://www.tumblr.com/share/link?url=http://vpekar.github.io/semantic-clustering-of-words-with-majorclust-and-word2vec.html&title=Semantic clustering of words with MajorClust and word2vec"
         onclick="window.open(this.href, 'add-to-get-pocket', 'width=490,height=530');return false;"
         title="add to tumblr" >
        <i class="fab fa-tumblr"></i>
      </a>

      <a class="pocket-add-button"
         href="https://getpocket.com/edit?url=http://vpekar.github.io/semantic-clustering-of-words-with-majorclust-and-word2vec.html&title=Semantic clustering of words with MajorClust and word2vec"
         onclick="window.open(this.href, 'add-to-get-pocket', 'width=490,height=530');return false;"
         title="add to pocket" >
        <i class="fab fa-get-pocket"></i>
      </a>


    </div>


</section>
</article>


  </main>
    <footer>
      <!--
      <div class="author__logo">
          <img src="../images/fb.jpg" alt="logo">
      </div>-->
      <section class="author">
        <!--<div class="author__name">
          <a href="http://vpekar.github.io/pages/about.html">Viktor Pekar</a>
          <p></p>
        </div> -->
        <div class="author__link">
          <ul>
            <li><a href="https://profile.codersrank.io/user/vpekar" title="Codersrank"><i class="fas fa-link"></i></a></li>
            <li>
              <a href="https://twitter.com/pekarv" target="_blank" title="twitter">
                <i class="fab fa-twitter-square"></i>
              </a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/vpekar/" target="_blank" title="linkedin">
                <i class="fab fa-linkedin"></i>
              </a>
            </li>
            <li>
              <a href="https://github.com/vpekar" target="_blank" title="github">
                <i class="fab fa-github-square"></i>
              </a>
            </li>
            <li>
              <a href="https://www.researchgate.net/profile/Viktor_Pekar" target="_blank" title="researchgate">
                <i class="fab fa-researchgate"></i>
              </a>
            </li>
            <li>
              <a href="https://scholar.google.co.uk/citations?user=wZg3myQAAAAJ&hl=en" target="_blank" title="googlescholar">
                <i class="ai ai-google-scholar-square"></i>
              </a>
            </li>
            <li>
              <a href="https://dblp.uni-trier.de/pers/hd/p/Pekar_0001:Viktor" target="_blank" title="dblp">
                <i class="ai ai-dblp-square"></i>
              </a>
            </li>
            <li>
              <a href="https://www.semanticscholar.org/author/Viktor-Pekar/32284161" target="_blank" title="semanticscholar">
                <i class="ai ai-semantic-scholar-square"></i>
              </a>
            </li>
            <li>
              <a href="http://vpekar.github.io/feed/all.rss.xml" target="_blank" title="Feed">
                <i class="fas fa-rss-square"></i>
              </a>
            </li>
          </ul>
        </div>
      </section>
      <div class="ending-message">
        <p>&copy; Viktor Pekar. Powered by <a href="http://getpelican.com" target="_blank">Pelican</a>, Theme is using <a href="https://github.com/laughk/pelican-hss" target="_blank">HSS</a>. </p>
      </div>
    </footer>
    <!-- Default Statcounter code for Vpekar.github.io
    http://vpekar.github.io/ -->
    <script type="text/javascript">
    var sc_project=11724865;
    var sc_invisible=1;
    var sc_security="f8dc32d7";
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js"
    async></script>
    <noscript><div class="statcounter"><a title="hit counter"
    href="http://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="//c.statcounter.com/11724865/0/f8dc32d7/1/" alt="hit
    counter"></a></div></noscript>
<!-- End of Statcounter Code -->
</body>
</html>